


import pandas as pd
import numpy as np
import spacy
from spacy import displacy
import networkx as nx
import os
import matplotlib.pyplot as plt
import scipy
import re


# Download English module
!python -m spacy download en_core_web_sm


# Load spacy English module
NER = spacy.load("en_core_web_sm")





# Import txt file
with open('Key_Events_20th_Century.txt', 'r', errors='ignore') as file:
    data = file.read().replace('\n', '')





# Clean text using re.sub
cleaned_text = re.sub(r'[^\w\s]', '', data.lower())


# Sentence tokenization
from nltk.tokenize import sent_tokenize
tokenized_sent = sent_tokenize(cleaned_text)
print(tokenized_sent)  # Inspect tokenized sentences

# Word tokenization
from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(cleaned_text)
print(tokenized_word)  # Inspect tokenized words


# Load country names from CSV
countries_data = pd.read_csv('countries_list_20th_century_1.5.csv')


countries_data.head()


# Dropping 'Unnamed: 0' Column
countries_data = countries_data.drop(columns=['Unnamed: 0'])
countries_data.head()


# Clean up the country names column and convert it to a list
countries_list = countries_data['country_name'].str.strip().str.lower().tolist()


# Tokenize the text
from nltk.tokenize import word_tokenize
dist_words = word_tokenize(cleaned_text)

# Convert the tokenized list into a single string
listToStr = ' '.join([str(elem).lower() for elem in dist_words])

# Clean the tokenized words from unwanted characters and count occurrences
from collections import Counter
all_counts = Counter(re.sub(r'\W+', ' ', listToStr).split())


# Count mentions of each country
country_mentions = {country: all_counts.get(country, 0) for country in countries_list}

# Convert the dictionary to a DataFrame and sort by mentions
import pandas as pd
country_mentions_df = pd.DataFrame(list(country_mentions.items()), columns=['Country', 'Mentions']).sort_values(by='Mentions', ascending=False)

# Display the DataFrame
print(country_mentions_df)


output_file_path = 'Cleaned_Key_Events_20th_Century.txt'

# Save the cleaned text to a .txt file
with open(output_file_path, 'w', encoding='utf-8') as file:
    file.write(cleaned_text)

print(f"Cleaned text has been saved to {output_file_path}")


# Import cleaned txt file
with open('Cleaned_Key_Events_20th_Century.txt', 'r', errors='ignore') as file:
    cleaned_data = file.read()





events = NER(cleaned_data)


# Visualize identified entities
displacy.render(events[273:20000], style = "ent", jupyter = True)





df_sentences = [] # empty shell to store results
# Loop through sentences, get entity list for each sentence
for sent in events.sents:
    entity_list = [ent.text for ent in sent.ents]
    df_sentences.append({"sentence": sent, "entities": entity_list})
    
df_sentences = pd.DataFrame(df_sentences)


df_sentences.head(10)





# Function to filter out entities not of interest
def filter_entity(ent_list, character_df):
    return [ent for ent in ent_list 
            if ent in list(countries_data['country_name'])]


# Apply the filter function to the 'entities' column
df_sentences['filtered_entities'] = df_sentences['entities'].apply(lambda ents: filter_entity(ents, countries_data))

# Display the resulting DataFrame with filtered entities
print(df_sentences.head())


# Function to filter out entities not matching country names
def filter_country_entities(sent):
    return [ent.text for ent in sent.ents if ent.label_ == "GPE" and ent.text.lower() in countries_list]

# Apply the filter to extract only country entities
df_sentences['filtered_entities'] = df_sentences['sentence'].apply(lambda sent: filter_country_entities(sent))

# Display the resulting DataFrame with filtered country entities
print(df_sentences.head())





# Set the window size for the sliding window
window_size = 5

# Initialize an empty list to store relationships
relationships = []

# Loop through the sentences with a sliding window approach
for i in range(len(df_sentences) - window_size + 1):
    # Determine the end index of the current window
    end_i = i + window_size
    
    # Collect all country entities from the current window of sentences
    country_entities = sum(df_sentences.loc[i:end_i, 'filtered_entities'], [])
    
    # Remove consecutive duplicates in the country entities
    unique_countries = [country_entities[j] for j in range(len(country_entities)) if (j == 0) or (country_entities[j] != country_entities[j-1])]
    
    # Record relationships between consecutive unique countries
    if len(unique_countries) > 1:
        for idx, source in enumerate(unique_countries[:-1]):
            target = unique_countries[idx + 1]
            relationships.append({"source": source, "target": target})

# Convert the list of relationships into a DataFrame
relationships_df = pd.DataFrame(relationships)

# Display the relationships DataFrame
print(relationships_df.head())


# Sort each row to ensure that 'source' and 'target' are in lexicographical order
relationships_df = pd.DataFrame(np.sort(relationships_df.values, axis=1), columns=relationships_df.columns)

# Display the first five rows of the sorted DataFrame
print(relationships_df.head())


# Add a 'value' column to indicate each relationship occurrence
relationships_df["value"] = 1

# Group by 'source' and 'target' and sum the 'value' to count occurrences
relationships_df = relationships_df.groupby(["source", "target"], sort=False, as_index=False).sum()

# Display the summarized relationships DataFrame
print(relationships_df.head())





relationships_df.to_csv('Key_Events_20th_Century_relationship.csv')
