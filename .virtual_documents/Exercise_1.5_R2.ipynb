


# Importing Libraries
from textblob import TextBlob
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import Counter
sns.set()





txt_file = open('Key_Events_20th_Century.txt', encoding='utf-8')


# Import txt file
with open('Key_Events_20th_Century.txt', 'r', errors='ignore') as file:
    data = file.read().replace('\n', '')


# Cleaning text: remove punctuation, numbers, and extra spaces; convert to lowercase
cleaned_text = re.sub(r'[^\w\s]', ' ', data)  # Remove punctuation and replace with space to avoid concatenation
cleaned_text = re.sub(r'[^a-zA-Z0-9\s]', '', cleaned_text)
cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()  # Replace multiple spaces with a single space and strip leading/trailing spaces
cleaned_text = re.sub(r'([a-z])([A-Z])', r'\1 \2', cleaned_text)  # Split camelCase
cleaned_text = re.sub(r'([a-zA-Z])(\d)', r'\1 \2', cleaned_text)  # Split letters and numbers
cleaned_text = re.sub(r'(\d)([a-zA-Z])', r'\1 \2', cleaned_text)  # Split numbers and letters
cleaned_text = cleaned_text.lower()  # Convert to lowercase





from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist


# Sentence tokenization 
from nltk.tokenize import sent_tokenize
tokenized_sent = sent_tokenize(cleaned_text)
print(tokenized_sent)


# Word tokenization
from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(cleaned_text)
print(tokenized_word)


# Create frequency distribution
from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print(dist_words)





# Get the 10 most common words
top_10_words = dist_words.most_common(10)


top_10_words


# Convert to DataFrame for plotting
df_top_10 = pd.DataFrame(top_10_words, columns=["Word", "Frequency"])


# Create a bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x="Frequency", y="Word", data=df_top_10, palette="coolwarm")
plt.title("Top 10 Most Common Words (Including Stop Words and Punctuation)")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.tight_layout()
plt.show()





from nltk.corpus import stopwords
import string
import re


# Define stop words and punctuation
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)


# Remove stop words and punctuation
filtered_tokens = [word for word in tokenized_word if word.lower() not in stop_words and word not in punctuation]

# Recalculate frequency distribution
filtered_dist_words = FreqDist(filtered_tokens)


# Get the 10 most common words after filtering
top_10_filtered = filtered_dist_words.most_common(10)

# Convert to DataFrame for visualization
df_top_10_filtered = pd.DataFrame(top_10_filtered, columns=["Word", "Frequency"])


top_10_filtered


# Plot the bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x="Frequency", y="Word", data=df_top_10_filtered, palette="viridis")
plt.title("Top 10 Most Common Words (Excluding Stop Words and Punctuation)")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.tight_layout()
plt.show()








# Create a TextBlob object from the tokenized words
text_blob = TextBlob(" ".join(filtered_dist_words))  # Join the filtered words into a single string

# Generate POS tags using TextBlob
tags_list = text_blob.tags

# Display the tags list
print(tags_list)





# Extract POS tags (second element in the tuple)
pos_tags = [tag[1] for tag in tags_list]

# Count the frequency of each POS tag
pos_counts = Counter(pos_tags)


pos_counts


# Get the 10 most common POS tags
top_10_pos = pos_counts.most_common(10)


top_10_pos


# Prepare the data for plotting
pos, frequencies = zip(*top_10_pos)





# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x=list(pos), y=list(frequencies), palette="Blues_d")
plt.xlabel('POS Tags')
plt.ylabel('Frequency')
plt.title('Top 10 POS Tags in the Article')
plt.xticks(rotation=45)
plt.show()





# Convert tags_list into a DataFrame for easier processing
tags_df = pd.DataFrame(tags_list, columns=['Word', 'Word type'])

# Filter and count for Nouns (NN, NNS, NNP, NNPS)
nouns_df = tags_df[tags_df['Word type'].str.startswith("NN")]
nouns_count = nouns_df.groupby('Word').size().reset_index(name='Occurrences')
nouns_sorted = nouns_count.sort_values(by='Occurrences', ascending=False).head(15)

# Filter and count for Verbs (VB, VBD, VBG, VBN, VBP, VBZ)
verbs_df = tags_df[tags_df['Word type'].str.startswith("VB")]
verbs_count = verbs_df.groupby('Word').size().reset_index(name='Occurrences')
verbs_sorted = verbs_count.sort_values(by='Occurrences', ascending=False).head(15)

# Filter and count for Adjectives (JJ, JJR, JJS)
adjectives_df = tags_df[tags_df['Word type'].str.startswith("JJ")]
adjectives_count = adjectives_df.groupby('Word').size().reset_index(name='Occurrences')
adjectives_sorted = adjectives_count.sort_values(by='Occurrences', ascending=False).head(15)


nouns_sorted



