


# Importing Libraries
from textblob import TextBlob
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import Counter
sns.set()





txt_file = open('Key_Events_20th_Century.txt', encoding='utf-8')


# Import txt file
with open('Key_Events_20th_Century.txt', 'r', errors='ignore') as file:
    data = file.read().replace('\n', '')


# Creating Path
path = r'C:\Users\anush\20th_Century'


# Loading the twentieth-century data from the CSV file
countries_data = pd.read_csv(os.path.join(path, 'countries_list_20th_century_1.5.csv'), index_col = False)

# Displaying the first few rows to verify the content
countries_data.head()


# Dropping 'Unnamed: 0' Column
countries_data = countries_data.drop(columns=['Unnamed: 0'])
countries_data.head()





from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist


# Sentence tokenization 
from nltk.tokenize import sent_tokenize
tokenized_sent = sent_tokenize(data)
print(tokenized_sent)


# Word tokenization
from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(data)
print(tokenized_word)


# Create frequency distribution
from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print(dist_words)





# Get the 10 most common words
top_10_words = dist_words.most_common(10)


top_10_words


# Convert to DataFrame for plotting
df_top_10 = pd.DataFrame(top_10_words, columns=["Word", "Frequency"])


# Create a bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x="Frequency", y="Word", data=df_top_10, palette="coolwarm")
plt.title("Top 10 Most Common Words (Including Stop Words and Punctuation)")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.tight_layout()
plt.show()





from nltk.corpus import stopwords
import string
import re


# Define stop words and punctuation
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)


# Remove stop words and punctuation
filtered_tokens = [word for word in tokenized_word if word.lower() not in stop_words and word not in punctuation]

# Recalculate frequency distribution
filtered_dist_words = FreqDist(filtered_tokens)


# Get the 10 most common words after filtering
top_10_filtered = filtered_dist_words.most_common(10)

# Convert to DataFrame for visualization
df_top_10_filtered = pd.DataFrame(top_10_filtered, columns=["Word", "Frequency"])


top_10_filtered


# Plot the bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x="Frequency", y="Word", data=df_top_10_filtered, palette="viridis")
plt.title("Top 10 Most Common Words (Excluding Stop Words and Punctuation)")
plt.xlabel("Frequency")
plt.ylabel("Words")
plt.tight_layout()
plt.show()








# Create a TextBlob object from the tokenized words
text_blob = TextBlob(" ".join(filtered_dist_words))  # Join the filtered words into a single string

# Generate POS tags using TextBlob
tags_list = text_blob.tags

# Display the tags list
print(tags_list)





# Extract POS tags (second element in the tuple)
pos_tags = [tag[1] for tag in tags_list]

# Count the frequency of each POS tag
pos_counts = Counter(pos_tags)


# Get the 10 most common POS tags
top_10_pos = pos_counts.most_common(10)


top_10_pos


# Prepare the data for plotting
pos, frequencies = zip(*top_10_pos)





# Create a bar plot using Seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x=list(pos), y=list(frequencies), palette="Blues_d")
plt.xlabel('POS Tags')
plt.ylabel('Frequency')
plt.title('Top 10 POS Tags in the Article')
plt.xticks(rotation=45)
plt.show()





# Convert tags_list into a DataFrame for easier processing
tags_df = pd.DataFrame(tags_list, columns=['Word', 'Word type'])

# Filter and count for Nouns (NN, NNS)
nouns_df = tags_df[(tags_df['Word type'] == "NN") | (tags_df['Word type'] == "NNS")]
nouns_count = nouns_df.groupby('Word').size().reset_index(name='Occurrences')
nouns_sorted = nouns_count.sort_values(by='Occurrences', ascending=False).head(15)

# Filter and count for Verbs (VB, VBD, VBG, VBN, VBP, VBZ)
verbs_df = tags_df[tags_df['Word type'].str.startswith("VB")]
verbs_count = verbs_df.groupby('Word').size().reset_index(name='Occurrences')
verbs_sorted = verbs_count.sort_values(by='Occurrences', ascending=False).head(15)

# Filter and count for Adjectives (JJ, JJR, JJS)
adjectives_df = tags_df[tags_df['Word type'].str.startswith("JJ")]
adjectives_count = adjectives_df.groupby('Word').size().reset_index(name='Occurrences')
adjectives_sorted = adjectives_count.sort_values(by='Occurrences', ascending=False).head(15)


nouns_sorted


verbs_sorted


adjectives_sorted


# Plotting the results
fig, axes = plt.subplots(3, 1, figsize=(10, 18))

# Plot for Nouns
sns.barplot(x='Occurrences', y='Word', data=nouns_sorted, ax=axes[0], palette='viridis')
axes[0].set_title('Top 15 Nouns by Occurrences')
axes[0].set_xlabel('Occurrences')
axes[0].set_ylabel('Nouns')

# Plot for Verbs
sns.barplot(x='Occurrences', y='Word', data=verbs_sorted, ax=axes[1], palette='coolwarm')
axes[1].set_title('Top 15 Verbs by Occurrences')
axes[1].set_xlabel('Occurrences')
axes[1].set_ylabel('Verbs')

# Plot for Adjectives
sns.barplot(x='Occurrences', y='Word', data=adjectives_sorted, ax=axes[2], palette='Blues')
axes[2].set_title('Top 15 Adjectives by Occurrences')
axes[2].set_xlabel('Occurrences')
axes[2].set_ylabel('Adjectives')

plt.tight_layout()
plt.show()








# Clean up the country names column and convert it to a list
countries_list = countries_data['country_name'].str.strip().str.lower().tolist()  # Convert 'country_name' column to lowercase list

# Convert the tokenized list into a single string
listToStr = ' '.join([str(elem).lower() for elem in filtered_dist_words])  # Join the list of words into a string and convert to lowercase


# Clean the tokenized words from unwanted characters and count occurrences
all_counts = Counter(re.sub(r'\W+', ' ', listToStr).split())  # Clean non-alphanumeric characters and split the text


all_counts


# Replace specific country name aliases if necessary
country_mentions = {country: all_counts.get(country, 0) for country in countries_list}  # Ensure no KeyError if country is not found


# Convert the set to a list to allow slicing
filtered_dist_words_list = list(filtered_dist_words)

# Print the first 20 tokenized words
print(filtered_dist_words_list[:20])


# Convert the dictionary into a DataFrame for easier manipulation
df_countries = pd.DataFrame(list(country_mentions.items()), columns=["Country", "Times Mentioned"])

# Sort the DataFrame by the 'Times Mentioned' column in descending order
df_sorted_countries = df_countries.sort_values(by="Times Mentioned", ascending=False)


df_sorted_countries





# Plot the frequency of the number of times countries are mentioned
plt.figure(figsize=(10, 6))
sns.barplot(x="Times Mentioned", y="Country", data=df_sorted_countries, palette="viridis")
plt.xticks(rotation=45)
plt.title('Frequency of Country Mentions in the Text')
plt.xlabel('Country')
plt.ylabel('Number of Mentions')
plt.show()





# Sort the DataFrame by "Times Mentioned" in descending order and select the top 30 countries
top_countries = df_sorted_countries.sort_values(by="Times Mentioned", ascending=False).head(30)

# Plot the frequency of mentions using a horizontal bar plot
plt.figure(figsize=(12, 10))  # Set the figure size
sns.barplot(x="Times Mentioned", y="Country", data=top_countries, palette="viridis")  # Create the bar plot

# Add plot labels and title
plt.title('Top 30 Countries Mentioned in the Text')  # Set the title of the plot
plt.xlabel('Times Mentioned')  # Label for the x-axis
plt.ylabel('Country')  # Label for the y-axis
plt.tight_layout()  # Adjust layout to avoid overlap

# Display the plot
plt.show()


top_countries











text_sent = TextBlob(str(tokenized_sent))


print(text_sent.sentiment)





# Sentiment output for a single sentence
polarity = 0.05533973606849814
subjectivity = 0.3747840064915302

# Data for plotting
scores = [polarity, subjectivity]
labels = ['Polarity', 'Subjectivity']

# Create a bar plot
plt.figure(figsize=(6, 4))
plt.bar(labels, scores, color=['blue', 'green'])
plt.title("Sentiment Analysis Scores")
plt.ylabel("Score Value")
plt.ylim(-1, 1)  # Setting y-axis to range from -1 to 1 for polarity and subjectivity

# Display the plot
plt.show()
